{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LabExercise3_solution.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"RPmP9p95UPCN","colab_type":"text"},"cell_type":"markdown","source":["# CO460 - Deep Learning - Lab exercise 3"]},{"metadata":{"id":"ZCeDdR-QUPCP","colab_type":"text"},"cell_type":"markdown","source":["## Introduction\n","\n","In this exercise, you will develop and experiment with convolutional AEs (CAE) and VAEs (CVAE).\n","You will be asked to:\n","\n","- experiment with the architectures and compare the convolutional models to the fully connected ones. \n","- investigate and implement sampling and interpolation in the latent space."]},{"metadata":{"id":"rA0n9o2uUPCQ","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image \n","import torch.nn.functional as F\n","from utils import *\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from utils import denorm_for_tanh, denorm_for_sigmoid"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EWmoGD1aUPCT","colab_type":"text"},"cell_type":"markdown","source":["### Device selection"]},{"metadata":{"id":"TJXU1HXGUPCU","colab_type":"code","outputId":"17cc5917-64b1-467a-e4f2-6cf37e97e1df","executionInfo":{"status":"ok","timestamp":1550522074878,"user_tz":0,"elapsed":482,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["GPU = True\n","device_idx = 0\n","if GPU:\n","    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n","else:\n","    \n","    device = torch.device(\"cpu\")\n","print(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"metadata":{"id":"Mcz8CpRdUPCW","colab_type":"text"},"cell_type":"markdown","source":["### Reproducibility"]},{"metadata":{"id":"i5QyaHNAUPCX","colab_type":"code","outputId":"e0e307f5-3c01-4180-a58c-6db88f8c2859","executionInfo":{"status":"ok","timestamp":1550522077444,"user_tz":0,"elapsed":472,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# We set a random seed to ensure that your results are reproducible.\n","if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","torch.manual_seed(0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f13e7f470b0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"Rbgl8zDVUPCZ","colab_type":"text"},"cell_type":"markdown","source":["## Part 1 - CAE"]},{"metadata":{"id":"SHaBXJw0UPCa","colab_type":"text"},"cell_type":"markdown","source":["### Normalization: \n","$ x_{norm} = \\frac{x-\\mu}{\\sigma} $\n","\n","_Thus_ :\n","$ \\min{x_{norm}} = \\frac{\\min{(x)}-\\mu}{\\sigma} = \\frac{0-0.5}{0.5} = -1 $\n","\n","_Similarly_:\n","\n","$ \\max{(x_{norm})} = ... = 1 $\n","\n","\n","* Input $\\in [-1,1] $\n","* Output should span the same interval $ \\rightarrow$ Activation function of the output layer should be chosen carfeully (Here??)"]},{"metadata":{"id":"HHjbVughUPCb","colab_type":"code","outputId":"367c0fb0-06e5-46cd-a774-83c26d5d526a","executionInfo":{"status":"ok","timestamp":1550523249386,"user_tz":0,"elapsed":3190,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["transform = transforms.Compose([\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","denorm = denorm_for_tanh\n","\n","train_dat = datasets.MNIST(\n","    \"data/\", train=True, download=True, transform=transform\n",")\n","test_dat = datasets.MNIST(\"data/\", train=False, transform=transform)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"8UWPE2yoUPCd","colab_type":"text"},"cell_type":"markdown","source":["### Hyper-parameter selection"]},{"metadata":{"id":"ac4h3ewfUPCe","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.exists('./CAE'):\n","    os.mkdir('./CAE')\n","    \n","num_epochs = 20\n","batch_size = 128\n","learning_rate = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mcTe745-UPCg","colab_type":"text"},"cell_type":"markdown","source":["### Define the dataloaders"]},{"metadata":{"id":"yU0J5UyKUPCh","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader = DataLoader(train_dat, batch_size, shuffle=True)\n","test_loader = DataLoader(test_dat, batch_size, shuffle=False)\n","\n","it = iter(test_loader)\n","sample_inputs, _ = next(it)\n","fixed_input = sample_inputs[:32, :, :, :]\n","\n","x1 = sample_inputs[0, :, :, :]\n","x2 = sample_inputs[1, :, :, :]\n","\n","in_dim = fixed_input.shape[-1]*fixed_input.shape[-2]\n","\n","save_image(fixed_input, './CAE/image_original.png')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bl6TeKIoUPCl","colab_type":"text"},"cell_type":"markdown","source":["### Define the model - CAE\n","\n","Complete the `encoder` and `decoder` methods in the CAE pipeline.\n","\n","To find an effective architecture, you can experiment with the following:\n","- the number of convolutional layers\n","- the kernels' sizes\n","- the stride values\n","- the size of the latent space layer"]},{"metadata":{"id":"GJ77VAnvUPCm","colab_type":"code","colab":{}},"cell_type":"code","source":["class CAE(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(CAE, self).__init__()\n","        \"\"\"\n","        TODO: Define here the layers (convolutions, relu etc.) that will be\n","        used in the encoder and decoder pipelines.\n","        \"\"\"\n","        # encoder layers\n","        self.conv1 = nn.Conv2d(1, 16, 3, stride=3, padding=1)\n","        self.maxp1 = nn.MaxPool2d(2, stride=2)\n","        self.conv2 = nn.Conv2d(16, 8, 3, stride=2, padding=1)\n","        self.maxp2 = nn.MaxPool2d(2, stride=1)\n","        self.fc = nn.Linear(32, latent_dim)\n","        \n","        # decoder layers\n","        self.deconv1 = nn.ConvTranspose2d(8, 16, 3, stride=2)\n","        self.deconv2 = nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1)\n","        self.deconv3 = nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1)\n","        self.tanh = nn.Tanh()\n","        \n","        # relu\n","        self.relu = nn.ReLU(True)\n","        \n","        \n","    def encode(self, x):\n","        \"\"\"\n","        TODO: Construct the encoder pipeline here. The encoder's\n","        output will be the laten space representation of x.\n","        \"\"\"\n","        x = self.relu(self.conv1(x)) # b, 16, 10, 10\n","        x = self.maxp1(x) # b, 16, 5, 5       \n","        x = self.relu(self.conv2(x)) # b, 8, 3, 3       \n","        x = self.maxp2(x) # b, 8, 2, 2\n","        x = x.view(x.size(0), -1) # b, 8*2*2\n","        x = self.fc(x) # b, h_dim\n","        return x\n","    \n","    def decode(self, z):\n","        \"\"\"\n","        TODO: Construct the decoder pipeline here. The decoder should \n","        generate an output tensor with equal dimenssions to the\n","        encoder's input tensor.\n","        \"\"\"\n","        z = z.view(z.size(0), 8, 2, 2) # b, 8, 2, 2\n","        z = self.relu(self.deconv1(z)) # b, 16, 5, 5\n","        z = self.relu(self.deconv2(z)) # b, 8, 15, 15\n","        z = self.tanh(self.deconv3(z)) # b, 1, 28, 28\n","        return z\n","        \n","        return z\n","\n","    def forward(self, x):\n","        x = self.encode(x)\n","        x = self.decode(x)\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b_UVeCR3Va6A","colab_type":"code","colab":{}},"cell_type":"code","source":["cv_AE = CAE(latent_dim=latent_dim)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z99U1L0JUPCo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Instantiate the model\n","latent_dim = 32\n","cv_AE = CAE(latent_dim=latent_dim)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NB0m7ieBUPCs","colab_type":"text"},"cell_type":"markdown","source":["### Define Loss function"]},{"metadata":{"id":"zX-d6_EXUPCs","colab_type":"code","colab":{}},"cell_type":"code","source":["criterion = nn.L1Loss(reduction='sum')  # can we use any other loss here?\n","def loss_function_CAE(recon_x, x):\n","    recon_loss = criterion(recon_x, x)\n","    return recon_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XxLjQFX5UPCu","colab_type":"text"},"cell_type":"markdown","source":["### Initialize Model and print number of parameters"]},{"metadata":{"id":"rfrakViEUPCv","colab_type":"code","outputId":"7c498a27-632b-41ce-d6e1-83ef9c1e080a","executionInfo":{"status":"ok","timestamp":1550523274805,"user_tz":0,"elapsed":456,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"cell_type":"code","source":["model = cv_AE.to(device)\n","params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total number of parameters is: {}\".format(params))  # what would the number actually be?\n","print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total number of parameters is: 6785\n","CAE(\n","  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n","  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (maxp2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=32, out_features=32, bias=True)\n","  (deconv1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n","  (deconv2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1))\n","  (deconv3): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n","  (tanh): Tanh()\n","  (relu): ReLU(inplace)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"cjgdjc33UPCy","colab_type":"text"},"cell_type":"markdown","source":["### Choose and initialize optimizer"]},{"metadata":{"id":"4qnKIhBxUPCy","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H-HLcHQgUPC0","colab_type":"text"},"cell_type":"markdown","source":["### Train"]},{"metadata":{"scrolled":true,"id":"cz_SezlrUPC1","colab_type":"code","outputId":"7ce51836-94ca-450f-c755-ed73ce42f019","executionInfo":{"status":"error","timestamp":1550523301141,"user_tz":0,"elapsed":2244,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":1166}},"cell_type":"code","source":["model.train()\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    for batch_idx, data in enumerate(train_loader):\n","        img, _ = data\n","        img = img.to(device)\n","        optimizer.zero_grad()\n","        # forward\n","        print(img.size())\n","        recon_batch = model(img)\n","        loss = loss_function_CAE(recon_batch, img)\n","        # backward\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","    # print out losses and save reconstructions for every epoch\n","    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, train_loss / len(train_loader.dataset)))\n","    recon = denorm(model(fixed_input.to(device)))\n","    save_image(recon, './CAE/reconstructed_epoch_{}.png'.format(epoch))\n","\n","# save the model\n","torch.save(model.state_dict(), './CAE/model.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n","torch.Size([128, 1, 28, 28])\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3f1bef1718c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrecon_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function_CAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-514a81e531f7>\u001b[0m in \u001b[0;36mloss_function_CAE\u001b[0;34m(recon_x, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# can we use any other loss here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function_CAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"0VEY4WtUUPC3","colab_type":"text"},"cell_type":"markdown","source":["### Test"]},{"metadata":{"id":"uWdvytc_UPC4","colab_type":"code","colab":{}},"cell_type":"code","source":["# load the model\n","model.load_state_dict(torch.load(\"./CAE/model.pth\"))\n","model.eval()\n","test_loss = 0\n","with torch.no_grad():\n","    for i, (img, _) in enumerate(test_loader):\n","        img = img.to(device)\n","        recon_batch = model(img)\n","        test_loss += loss_function_CAE(recon_batch, img)\n","    # reconstruct and save the last batch\n","    recon_batch = model(recon_batch.to(device))\n","    img = denorm(img.cpu())\n","    # save the original last batch\n","    save_image(img, './CAE/test_original.png')\n","    save_image(denorm(recon_batch.cpu()), './CAE/reconstructed_test.png')\n","    # loss calculated over the whole test set\n","    test_loss /= len(test_loader.dataset)\n","    print('Test set loss: {:.4f}'.format(test_loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7MVI660MUPC7","colab_type":"text"},"cell_type":"markdown","source":["### Interpolations"]},{"metadata":{"id":"LlX9aF-dUPC8","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define inpute tensors\n","x1 = sample_inputs[10, :, :, :]\n","x2 = sample_inputs[36, :, :, :]\n","\n","# Create the latent representations\n","z1 = model.encode(x1.to(device)[None,:]).cpu().detach().numpy()[0]\n","z2 = model.encode(x2.to(device)[None,:]).cpu().detach().numpy()[0]\n","\n","\"\"\"\n","TODO: Find a way to create interpolated results from the CAE.\n","\"\"\"\n","Z = interpolate(z1, z2, num=11)\n","Z = torch.FloatTensor(Z.T).cuda()\n","X_hat = model.decode(Z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c81H3bUXUPC_","colab_type":"code","colab":{}},"cell_type":"code","source":["save_image(denorm(X_hat.cpu()), './CAE/interpolation.png')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TJR2btZ-UPDB","colab_type":"text"},"cell_type":"markdown","source":["## Part 2 - CVAE"]},{"metadata":{"id":"ijM8AyJWUPDC","colab_type":"text"},"cell_type":"markdown","source":["### Normalization"]},{"metadata":{"id":"BOawc9MEUPDC","colab_type":"code","colab":{}},"cell_type":"code","source":["transform = transforms.Compose([\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","denorm = denorm_for_tanh\n","\n","train_dat = datasets.MNIST(\n","    \"data/\", train=True, download=True, transform=transform\n",")\n","test_dat = datasets.MNIST(\"data/\", train=False, transform=transform)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Maog9PDHUPDF","colab_type":"text"},"cell_type":"markdown","source":["### Hyper-parameter selection"]},{"metadata":{"id":"dKR4V7fjUPDF","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.exists('./CVAE'):\n","    os.mkdir('./CVAE')\n","    \n","num_epochs = 20\n","batch_size = 128\n","learning_rate = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NQ3pDukdUPDH","colab_type":"text"},"cell_type":"markdown","source":["### Define the dataloaders"]},{"metadata":{"id":"TzTlnetWUPDI","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader = DataLoader(train_dat, batch_size, shuffle=True)\n","test_loader = DataLoader(test_dat, batch_size, shuffle=False)\n","\n","it = iter(test_loader)\n","sample_inputs, _ = next(it)\n","fixed_input = sample_inputs[:32, :, :, :]\n","\n","in_dim = fixed_input.shape[-1]*fixed_input.shape[-2]\n","\n","save_image(fixed_input, './CVAE/image_original.png')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dOux6dlTUPDK","colab_type":"text"},"cell_type":"markdown","source":["### Define the model - CVAE\n","\n","Complete the `encoder` and `decoder` methods in the CVAE pipeline.\n","\n","To find an effective architecture, you can experiment with the following:\n","- the number of convolutional layers\n","- the kernels' sizes\n","- the stride values\n","- the size of the latent space layer"]},{"metadata":{"id":"mNRqtUIjUPDL","colab_type":"code","colab":{}},"cell_type":"code","source":["# For the CVAE use the same architecture as in the CAE. \n","# You also need to implement reparameterization (also included in tutorial3)\n","\n","class CVAE(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(CVAE, self).__init__()\n","        \"\"\"\n","        TODO: Define here the layers (convolutions, relu etc.) that will be\n","        used in the encoder and decoder pipelines.\n","        \"\"\"\n","        \n","        \n","    def encode(self, x):\n","        \"\"\"\n","        TODO: Construct the encoder pipeline here.        \n","        \"\"\"\n","\n","        return mu, logvar\n","\n","    def reparametrize(self, mu, logvar):\n","        \"\"\"\n","        TODO: Implement reparameterization here.\n","        \"\"\"\n","\n","        return z\n","\n","    def decode(self, z):\n","        \"\"\"\n","        TODO: Construct the decoder pipeline here.        \n","        \"\"\"\n","\n","        return z\n"," \n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparametrize(mu, logvar)\n","        x_hat = self.decode(z)\n","        return x_hat, mu, logvar"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ab734Hi0UPDN","colab_type":"code","colab":{}},"cell_type":"code","source":["# Instantiate the model\n","latent_dim = \n","cv_VAE = CVAE(latent_dim =latent_dim)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NPzqi1mtUPDP","colab_type":"text"},"cell_type":"markdown","source":["### Define Loss function"]},{"metadata":{"id":"hQ7lKdqYUPDP","colab_type":"code","colab":{}},"cell_type":"code","source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function_VAE(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BsMifLJIUPDS","colab_type":"text"},"cell_type":"markdown","source":["### Initialize Model and print number of parameters"]},{"metadata":{"id":"eYgAbHGqUPDT","colab_type":"code","colab":{}},"cell_type":"code","source":["model = cv_AE.to(device)\n","params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total number of parameters is: {}\".format(params))  # what would the number actually be?\n","print(model)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TYF9-k3TUPDW","colab_type":"text"},"cell_type":"markdown","source":["### Choose and initialize optimizer"]},{"metadata":{"id":"WemsGFh3UPDW","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9h9PLzUxUPDZ","colab_type":"text"},"cell_type":"markdown","source":["### Train"]},{"metadata":{"scrolled":true,"id":"9PPB18LTUPDa","colab_type":"code","colab":{}},"cell_type":"code","source":["model.train()\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    for batch_idx, data in enumerate(train_loader):\n","        img, _ = data\n","        img = img.to(device)\n","        optimizer.zero_grad()\n","        # forward\n","        recon_batch = model(img)\n","        loss = loss_function_CAE(recon_batch, img)\n","        # backward\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","    # print out losses and save reconstructions for every epoch\n","    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, train_loss / len(train_loader.dataset)))\n","    recon = denorm(model(fixed_input.to(device)))\n","    save_image(recon, './CVAE/reconstructed_epoch_{}.png'.format(epoch))\n","\n","# save the model\n","torch.save(model.state_dict(), './CVAE/model.pth')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J-IyInlLUPDc","colab_type":"text"},"cell_type":"markdown","source":["### Test"]},{"metadata":{"id":"2p3kRLeTUPDc","colab_type":"code","colab":{}},"cell_type":"code","source":["# load the model\n","model.load_state_dict(torch.load(\"./CVAE/model.pth\"))\n","model.eval()\n","test_loss = 0\n","with torch.no_grad():\n","    for i, (img, _) in enumerate(test_loader):\n","        img = img.to(device)\n","        recon_batch = model(img)\n","        test_loss += loss_function_CAE(recon_batch, img)\n","    # reconstruct and save the last batch\n","    recon_batch = model(recon_batch.to(device))\n","    img = denorm(img.cpu())\n","    # save the original last batch\n","    save_image(img, './CVAE/test_original.png')\n","    save_image(denorm(recon_batch.cpu()), './CVAE/reconstructed_test.png')\n","    # loss calculated over the whole test set\n","    test_loss /= len(test_loader.dataset)\n","    print('Test set loss: {:.4f}'.format(test_loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YkEDEBIQUPDe","colab_type":"text"},"cell_type":"markdown","source":["### Sample"]},{"metadata":{"id":"suE6bY6jUPDf","colab_type":"text"},"cell_type":"markdown","source":["Sample the latent space and use the `decoder` to generate resutls."]},{"metadata":{"id":"TF8YO2u2UPDf","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_state_dict(torch.load(\"./CVAE/model.pth\"))\n","model.eval()\n","with torch.no_grad():\n","    \"\"\"\n","    TODO: Investigate how to sample the latent space of the CVAE.\n","    \"\"\"\n","    z = torch.randn(32, latent_dim).to(device)\n","    sample = model.decode(z)\n","    save_image(denorm(sample).cpu(), './CVAE/samples_' + '.png')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ii9tnYEQUPDh","colab_type":"text"},"cell_type":"markdown","source":["### Interpolations"]},{"metadata":{"id":"aXBAyUoUUPDi","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define inpute tensors\n","x1 = sample_inputs[10, :, :, :]\n","x2 = sample_inputs[36, :, :, :]\n","\n","# Create the latent representations\n","z1 = model.encode(x1.to(device)[None,:]).cpu().detach().numpy()[0]\n","z2 = model.encode(x2.to(device)[None,:]).cpu().detach().numpy()[0]\n","\n","\"\"\"\n","TODO: Find a way to create interpolated results from the CAE.\n","\"\"\"\n","Z = interpolate(z1, z2, num=11)\n","Z = torch.FloatTensor(Z.T).cuda()\n","X_hat = model.decode(Z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OPKPRpwIUPDj","colab_type":"code","colab":{}},"cell_type":"code","source":["save_image(denorm(X_hat.cpu()), './CAE/interpolation.png')"],"execution_count":0,"outputs":[]}]}