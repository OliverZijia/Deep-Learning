{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"questions.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"gXs2XCUR_tSN","colab_type":"text"},"cell_type":"markdown","source":["# Coursework 1: ML basics and fully-connected networks\n","\n","#### Instructions\n","\n","Please submit a version of this notebook containing your answers on CATe as *CW1*. Write your answers in the cells below each question.\n","\n","We recommend that you work on the Ubuntu workstations in the lab. This assignment and all code were only tested to work on these machines. In particular, we cannot guarantee compatibility with Windows machines and cannot promise support if you choose to work on a Windows machine.\n","\n","You can work from home and use the lab workstations via ssh (for list of machines: https://www.doc.ic.ac.uk/csg/facilities/lab/workstations). \n","\n","Once logged in, run the following commands in the terminal to set up a Python environment with all the packages you will need.\n","\n","    export PYTHONUSERBASE=/vol/bitbucket/nuric/pypi\n","    export PATH=/vol/bitbucket/nuric/pypi/bin:$PATH\n","\n","Add the above lines to your `.bashrc` to have these enviroment variables set automatically each time you open your bash terminal.\n","\n","Any code that you submit will be expected to run in this environment. Marks will be deducted for code that fails to run.\n","\n","Run `jupyter-notebook` in the coursework directory to launch Jupyter notebook in your default browser.\n","\n","DO NOT attempt to create a virtualenv in your home folder as you will likely exceed your file quota.\n","\n","**DEADLINE: 7pm, Tuesday 5th February, 2019**"]},{"metadata":{"id":"LglPxaUq_tSO","colab_type":"text"},"cell_type":"markdown","source":["## Part 1\n","\n","1. Describe two practical methods used to estimate a supervised learning model's performance on unseen data. Which strategy is most commonly used in most deep learning applications, and why?\n","2. Suppose that you have reason to believe that your multi-layer fully-connected neural network is overfitting. List four things that you could try to improve generalization performance."]},{"metadata":{"id":"wHkqSt5c_tSP","colab_type":"text"},"cell_type":"markdown","source":["## Part 1(Answer) add description\n","\n","1. 1 Estimate performance on unseen data\n","\n","\n","*   Holdout. We split up our training dataset into a ‘train’ and ‘test’ set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data. A common split when using the hold-out method is using 80% of data for training and the remaining 20% of the data for testing.\n","*   Cross validation, estimate the performance using validation set. In K Fold cross validation, we split our trainging dataset into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the  validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. \n","\n","\n","1. 2 Most Commonly used\n","\n","\n","*   Cross validation. Because it can effectively avoid overfitting, and can effectively use the data,  as can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method.  Although when the dataset is large or when the NN is deep, it's expensive to use this method, but in practice, data is also valuable. So we usually can't get a very large dataset.\n","\n","\n","2. How to avoid overfitting\n","\n","\n","*   Add regularizer to the loss function. Regularizer is seen to be a penalty term in the loss funtion, penalise the increasing of the parameter number.\n","*   Dropout. Randomly dropout some neurons during the traing process can effectively avoid the overfitting.\n","*   Train with more data(Data augmentation). Using more training data can, in most cases, lead to a better result, and can also mitigate the overfitting problem if we don't add more parameters again.\n","*   Early stopping. Terminate before reaching convergence, the effects is similar to decay regularization.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"Agvpv-90_tSQ","colab_type":"text"},"cell_type":"markdown","source":["## Part 2\n","\n","1. Why can gradient-based learning be difficult when using the sigmoid or hyperbolic tangent functions as hidden unit activation functions in deep, fully-connected neural networks?\n","2. Why is the issue that arises in the previous question less of an issue when using such functions as output unit activation functions, provided that an appropriate loss function is used?\n","3. What would happen if you initialize all the weights to zero in a multi-layer fully-connected neural network and attempt to train your model using gradient descent? What would happen if you did the same thing for a logistic regression model?"]},{"metadata":{"id":"OtTXByc1_tSS","colab_type":"text"},"cell_type":"markdown","source":["## Part 2(Answers)\n","\n","1. Disadvantages of sigmoid and hyperbolic tangent funtions in deep neural networks.\n","\n","\n","*   Because It suffers from vaninshing or exploding gradients issue. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Then if we use BP in a deep NN, layers deep in large networks using these nonlinear activation functions fail to receive useful gradient information.The amount of error decreases dramatically with each additional layer through which it is propagated, given the derivative of the chosen activation function.\n","\n","\n","2. Effect of the appropriate loss funtion\n","\n","\n","*   If we use an appropriate loss function, say, cross entropy, then due to the chain rule, when we backpropagate the gradient, the gradient of the activation function is multiplied by the  gradient of loss function,  then for cross entropy, we can get the derivative of loss function: pi-yi, which can obviously solve the influence of the gradient vanishing and gradient exploding.Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution really is. \n","\n","\n","3. Initialization issue\n","\n","\n","*  First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values. You can't do that if they all start at zero.\n","\n","*  Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another."]},{"metadata":{"id":"ZKE5FRYQ_tST","colab_type":"text"},"cell_type":"markdown","source":["## Part 3\n","\n","In this part, you will use PyTorch to implement and train a multinomial logistic regression model to classify MNIST digits.\n","\n","Restrictions:\n","* You must use (but not modify) the code provided in `utils.py`. **This file is deliberately not documented**; read it carefully as you will need to understand what it does to complete the tasks.\n","* You are NOT allowed to use the `torch.nn` module.\n","\n","Please insert your solutions to the following tasks in the cells below:\n","1. Complete the `MultinomialLogisticRegressionClassifier` class below by filling in the missing parts (expected behaviour is prescribed in the documentation):\n","    * The constructor\n","    * `forward`\n","    * `parameters`\n","    * `l1_weight_penalty`\n","    * `l2_weight_penalty`\n","\n","2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 92% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n"]},{"metadata":{"id":"lMV0mczB_tSU","colab_type":"code","colab":{}},"cell_type":"code","source":["from utils import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IZXACs7x_tSY","colab_type":"code","colab":{}},"cell_type":"code","source":["# *CODE FOR PART 3.1 IN THIS CELL*\n","import numpy as np\n","import torch\n","from torch.distributions import normal\n","\n","class MultinomialLogisticRegressionClassifier:\n","    def __init__(self, weight_init_sd=100.0):\n","        \"\"\"\n","        Initializes model parameters to values drawn from the Normal\n","        distribution with mean 0 and standard deviation `weight_init_sd`.\n","        \"\"\"\n","        self.weight_init_sd = weight_init_sd\n","\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        In = 784 # 28 * 28\n","        Out = 10\n","        m = normal.Normal(torch.Tensor([0.0]), torch.Tensor([weight_init_sd]))\n","        self.w = m.sample(sample_shape = torch.Size([In, Out])).view(In, Out)\n","        self.w.requires_grad_()\n","        self.b = torch.zeros((1, 10))\n","        self.b.requires_grad_()\n","        #self.theta = torch.cat((self.w,self.b),0)\n","        #self.theta.requires_grad_()  \n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","\n","    def __call__(self, *args, **kwargs):\n","        return self.forward(*args, **kwargs)\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Performs the forward pass through the model.\n","        \n","        Expects `inputs` to be a Tensor of shape (batch_size, 1, 28, 28) containing\n","        minibatch of MNIST images.\n","        \n","        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n","        before being fed into the model.\n","        \n","        Should return a Tensor of logits of shape (batch_size, 10).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        # flatten the data\n","        flat_X = inputs.reshape([-1,784])\n","        #flat_X = torch.cat((flat_X, self.b), 0)\n","        h = flat_X.matmul(self.w) + self.b\n","        h_row_max = torch.max(h, dim=1, keepdim = True)[0]\n","        h = h - h_row_max\n","        logits = h - torch.log(torch.sum(torch.exp(h)))\n","        # h = self.b + w_X\n","        # print(h.shape)\n","        # logits = torch.log(torch.exp(h) / torch.sum(torch.exp(h)))\n","\n","        return logits\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","\n","    def parameters(self):\n","        \"\"\"\n","        Should return an iterable of all the model parameter Tensors.\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        self.para = torch.cat((self.w,self.b),0)\n","        \n","        return [self.w, self.b]\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","        \n","    def l1_weight_penalty(self):\n","        \"\"\"\n","        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n","        of absolute values of all model parameters).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        #torch.Tensor(self.para)\n","        l1 = self.para.abs().sum()\n","        return l1\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","\n","    def l2_weight_penalty(self):\n","        \"\"\"\n","        Computes and returns the L2 weight penalty (i.e. \n","        sum of squared values of all model parameters).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        l2 = self.para.pow(2).sum()\n","        return l2\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"shj4jpoY_tSa","colab_type":"code","outputId":"b4e61356-95e6-4ca1-dc3d-ca7604abba99","executionInfo":{"status":"ok","timestamp":1549118369610,"user_tz":0,"elapsed":70233,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":773}},"cell_type":"code","source":["# *CODE FOR PART 3.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n","model = MultinomialLogisticRegressionClassifier(weight_init_sd=0.002)\n","res = run_experiment(\n","    model,\n","    optimizer=optim.SGD(model.parameters(), lr = 0.005, momentum = 0.8),\n","    train_loader=train_loader_1,\n","    val_loader=val_loader_1,\n","    test_loader=test_loader_1,\n","    n_epochs=10,\n","    l1_penalty_coef=0,\n","    l2_penalty_coef=0.01,\n","    suppress_output=False\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0: training...\n","Train set:\tAverage loss: 4.5922, Accuracy: 0.8911\n","Validation set:\tAverage loss: 7.2672, Accuracy: 0.9032\n","\n","Epoch 1: training...\n","Train set:\tAverage loss: 4.4921, Accuracy: 0.9119\n","Validation set:\tAverage loss: 7.2511, Accuracy: 0.9110\n","\n","Epoch 2: training...\n","Train set:\tAverage loss: 4.4750, Accuracy: 0.9166\n","Validation set:\tAverage loss: 7.2388, Accuracy: 0.9122\n","\n","Epoch 3: training...\n","Train set:\tAverage loss: 4.4667, Accuracy: 0.9198\n","Validation set:\tAverage loss: 7.2363, Accuracy: 0.9140\n","\n","Epoch 4: training...\n","Train set:\tAverage loss: 4.4601, Accuracy: 0.9221\n","Validation set:\tAverage loss: 7.2344, Accuracy: 0.9138\n","\n","Epoch 5: training...\n","Train set:\tAverage loss: 4.4553, Accuracy: 0.9239\n","Validation set:\tAverage loss: 7.2325, Accuracy: 0.9157\n","\n","Epoch 6: training...\n","Train set:\tAverage loss: 4.4524, Accuracy: 0.9243\n","Validation set:\tAverage loss: 7.2333, Accuracy: 0.9127\n","\n","Epoch 7: training...\n","Train set:\tAverage loss: 4.4482, Accuracy: 0.9254\n","Validation set:\tAverage loss: 7.2285, Accuracy: 0.9182\n","\n","Epoch 8: training...\n","Train set:\tAverage loss: 4.4463, Accuracy: 0.9264\n","Validation set:\tAverage loss: 7.2313, Accuracy: 0.9192\n","\n","Epoch 9: training...\n","Train set:\tAverage loss: 4.4441, Accuracy: 0.9262\n","Validation set:\tAverage loss: 7.2264, Accuracy: 0.9163\n","\n","Test set:\tAverage loss: 7.1993, Accuracy: 0.9248\n","\n"],"name":"stdout"}]},{"metadata":{"id":"CsHjhfht_tSd","colab_type":"text"},"cell_type":"markdown","source":["## Part 4\n","\n","In this part, you will use PyTorch to implement and train a multi-layer fully-connected neural network to classify MNIST digits.\n","\n","Your network must have three hidden layers with 128, 64, and 32 hidden units respectively.\n","\n","The same restrictions as in Part 3 apply.\n","\n","Please insert your solutions to the following tasks in the cells below:\n","1. Complete the `MultilayerClassifier` class below by filling in the missing parts of the following methods (expected behaviour is prescribed in the documentation):\n","\n","    * The constructor\n","    * `forward`\n","    * `parameters`\n","    * `l1_weight_penalty`\n","    * `l2_weight_penalty`\n","\n","2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 97% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n","\n","3. Describe an alternative strategy for initializing weights that may perform better than the strategy we have used here."]},{"metadata":{"id":"qJyd8W_h_tSe","colab_type":"code","colab":{}},"cell_type":"code","source":["# *CODE FOR PART 4.1 IN THIS CELL*\n","\n","class MultilayerClassifier:\n","    def __init__(self, activation_fun=\"sigmoid\", weight_init_sd=1.0):\n","        \"\"\"\n","        Initializes model parameters to values drawn from the Normal\n","        distribution with mean 0 and standard deviation `weight_init_sd`.\n","        \"\"\"\n","        super().__init__()\n","        self.activation_fun = activation_fun\n","        self.weight_init_sd = weight_init_sd\n","\n","        if self.activation_fun == \"relu\":\n","            self.activation = F.relu\n","        elif self.activation_fun == \"sigmoid\":\n","            self.activation = torch.sigmoid\n","        elif self.activation_fun == \"tanh\":\n","            self.activation = torch.tanh\n","        else:\n","            raise NotImplementedError()\n","\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        In = 784 # 28 * 28\n","        Hidden1 = 128\n","        Hidden2 = 64\n","        Hidden3 = 32\n","        Out = 10\n","        m = normal.Normal(torch.Tensor([0.0]), torch.Tensor([weight_init_sd]))\n","        self.w1 = m.sample(sample_shape = torch.Size([In, Hidden1])).view(In, Hidden1)\n","        self.w1.requires_grad_()\n","        self.b1 = torch.zeros((1, Hidden1))\n","        self.b1.requires_grad_()\n","        \n","        self.w2 = m.sample(sample_shape = torch.Size([Hidden1, Hidden2])).view(Hidden1, Hidden2)\n","        self.w2.requires_grad_()\n","        self.b2 = torch.zeros((1, Hidden2))\n","        self.b2.requires_grad_()\n","        \n","        self.w3 = m.sample(sample_shape = torch.Size([Hidden2, Hidden3])).view(Hidden2, Hidden3)\n","        self.w3.requires_grad_()\n","        self.b3 = torch.zeros((1, Hidden3))\n","        self.b3.requires_grad_()\n","        \n","        self.w4 = m.sample(sample_shape = torch.Size([Hidden3, Out])).view(Hidden3, Out)\n","        self.w4.requires_grad_()\n","        self.b4 = torch.zeros((1, Out))\n","        self.b4.requires_grad_()\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","\n","    def __call__(self, *args, **kwargs):\n","        return self.forward(*args, **kwargs)\n","     \n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Performs the forward pass through the model.\n","        \n","        Expects `inputs` to be Tensor of shape (batch_size, 1, 28, 28) containing\n","        minibatch of MNIST images.\n","        \n","        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n","        before being fed into the model.\n","        \n","        Should return a Tensor of logits of shape (batch_size, 10).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        flat_X = inputs.reshape([-1,784])\n","        \n","        h1 = flat_X.matmul(self.w1) + self.b1\n","        h1 = self.activation(h1)\n","        \n","        h2 = h1.matmul(self.w2) + self.b2\n","        h2 = self.activation(h2)\n","        \n","        h3 = h2.matmul(self.w3) + self.b3\n","        h3 = self.activation(h3)\n","        \n","        h = h3.matmul(self.w4) + self.b4\n","        \n","        h_row_max = torch.max(h, dim=1, keepdim = True)[0]\n","        h = h - h_row_max\n","        logits = h - torch.log(torch.sum(torch.exp(h))) \n","        \n","        '''\n","                logits = torch.log(torch.exp(h) / torch.sum(torch.exp(h)))\n","       \n","        '''\n","\n","    \n","        return logits\n","        #######################################################################\n","        #                       *self.w,self.b* END OF YOUR CODE **\n","        #######################################################################\n","\n","    def parameters(self):\n","        \"\"\"\n","        Should return an iterable of all the model parameter Tensors.\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        return [self.w1, self.b1, self.w2, self.b2,self.w3, self.b3, self.w4, self.b4]\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","        \n","    \n","    def l1_weight_penalty(self):\n","        \"\"\"\n","        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n","        of absolute values of all model parameters).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        l1 = 0\n","        for para in self.parameters():\n","          l1 += torch.sum(torch.abs(para))\n","          \n","        return l1\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n","\n","    def l2_weight_penalty(self):\n","        \"\"\"\n","        Computes and returns the L2 weight penalty (i.e. \n","        sum of squared values of all model parameters).\n","        \"\"\"\n","        #######################################################################\n","        #                       ** START OF YOUR CODE **\n","        #######################################################################\n","        l2 = 0\n","        for para in self.parameters():\n","          l2 += torch.sum(para ** 2)\n","          \n","        return l2\n","        #######################################################################\n","        #                       ** END OF YOUR CODE **\n","        #######################################################################\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SfSRzyOG_tSj","colab_type":"code","outputId":"6534b0ad-483c-4092-9d5a-d10cb9b6b279","executionInfo":{"status":"ok","timestamp":1549125514471,"user_tz":0,"elapsed":85352,"user":{"displayName":"ZIJIA WANG","photoUrl":"","userId":"14159745238755222819"}},"colab":{"base_uri":"https://localhost:8080/","height":773}},"cell_type":"code","source":["# *CODE FOR PART 4.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n","\n","model = MultilayerClassifier(activation_fun='relu', weight_init_sd=0.03)\n","res = run_experiment(\n","    model,\n","    optimizer=optim.SGD(model.parameters(),lr = 0.015, momentum = 0.8),\n","    train_loader=train_loader_1,\n","    val_loader=val_loader_1,\n","    test_loader=test_loader_1,\n","    n_epochs=10,\n","    l1_penalty_coef=0,\n","    l2_penalty_coef=0.001,\n","    suppress_output=False\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0: training...\n","Train set:\tAverage loss: 5.2343, Accuracy: 0.6425\n","Validation set:\tAverage loss: 7.2316, Accuracy: 0.9123\n","\n","Epoch 1: training...\n","Train set:\tAverage loss: 4.3813, Accuracy: 0.9407\n","Validation set:\tAverage loss: 7.1135, Accuracy: 0.9430\n","\n","Epoch 2: training...\n","Train set:\tAverage loss: 4.3034, Accuracy: 0.9617\n","Validation set:\tAverage loss: 7.0679, Accuracy: 0.9567\n","\n","Epoch 3: training...\n","Train set:\tAverage loss: 4.2704, Accuracy: 0.9690\n","Validation set:\tAverage loss: 7.0375, Accuracy: 0.9613\n","\n","Epoch 4: training...\n","Train set:\tAverage loss: 4.2522, Accuracy: 0.9740\n","Validation set:\tAverage loss: 7.0460, Accuracy: 0.9590\n","\n","Epoch 5: training...\n","Train set:\tAverage loss: 4.2387, Accuracy: 0.9777\n","Validation set:\tAverage loss: 7.0341, Accuracy: 0.9652\n","\n","Epoch 6: training...\n","Train set:\tAverage loss: 4.2315, Accuracy: 0.9801\n","Validation set:\tAverage loss: 7.0391, Accuracy: 0.9638\n","\n","Epoch 7: training...\n","Train set:\tAverage loss: 4.2256, Accuracy: 0.9815\n","Validation set:\tAverage loss: 7.0197, Accuracy: 0.9697\n","\n","Epoch 8: training...\n","Train set:\tAverage loss: 4.2200, Accuracy: 0.9831\n","Validation set:\tAverage loss: 7.0190, Accuracy: 0.9662\n","\n","Epoch 9: training...\n","Train set:\tAverage loss: 4.2153, Accuracy: 0.9841\n","Validation set:\tAverage loss: 7.0173, Accuracy: 0.9678\n","\n","Test set:\tAverage loss: 6.9963, Accuracy: 0.9746\n","\n"],"name":"stdout"}]},{"metadata":{"id":"6gvLeF8F_tSn","colab_type":"text"},"cell_type":"markdown","source":["4.3 In the above case, we initialise all weights using same distributions, to get a better result:\n","\n","*   We can intialise parameters of each layer seperately.\n","*   More concretely,  we can use He-et-al Initialization, the weights are initialized keeping in mind the size of the previous layer which helps in attaining a global minimum of the cost function faster and more efficiently.\n","\n"]}]}